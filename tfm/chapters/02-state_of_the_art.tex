\chapter{Revisión del estado de la técnica}

Antes de poder aventurarnos el desarrollo de una herramienta para la
orquestación de tareas de tiempo real usando contenedores, es necesario
estudiar y comprender los fundamentos y peculiaridades de diversos campos como
son los sistemas de tiempo real, la planificación de procesos, la computación en
la nube o las tecnologías de virtualización. En este capítulo, se introducen los
conceptos más importantes de estos campos, además de revisar otros trabajos
realizados con el objetivo de comprobar cuál es el estado de la investigación
en dichos temas.

\section{Sistemas empotrados, confiables y de criticalidad mixta}

Los ordenadores y teléfonos inteligentes que usamos a diario nos permiten
realizar multitud de tareas diferentes: desde leer el correo o navegar por
internet hasta editar imágenes y vídeos o realizar videollamadas. Aunque no nos
demos cuenta, interactuamos habitualmente con muchos más sistemas informáticos
además de los ya mencionados. La computación está presente en los coches, los
trenes, los aviones, los satélites espaciales, los televisores o las lavadoras.
Estos sistemas, que reciben el nombre de sistemas empotrados, son diseñados para
llevar a cabo de manera óptima un conjunto de tareas específico, frente al
enfoque generalista de los ordenadores de uso personal. Como se introduce en
\cite{wolf_high-performance_2014}, los sistemas empotrados son comunes en
contextos en los que el rendimiento es primordial, como son las comunicaciones
de red o la compresión/decompresión de audio y vídeo para retransmisiones en
directo. En este libro se exponen diversas aplicaciones de los sistemas
empotrados de alto rendimiento para sistemas ciber-físicos o CPS
(\textit{Cyber-Physical Systems}). Los CPS son, en esencia, sistemas
informáticos que interactúan con procesos físicos, actuando en función de los
cambios en su entorno. Este aspecto hace que el diseño y la implementación de
los CPS difiera considerablemente del resto de sistemas empotrados, ya que hay
que prestar especial atención a las características del entorno en el que se
desplegará el sistema y a los mecanismos de entrada y salida (interacción con el
entorno), además de optimizar el consumo de memoria y procesamiento para cumplir
con las limitaciones del hardware \cite{lee_introduction_2016}.

En algunos casos, estos sistemas son críticos, lo que significa que un fallo en
su funcionamiento supone daños graves a las personas o al medio ambiente. Este
es el caso de los aviones, los trenes o las plantas nucleares, por ejemplo. En
estos sistemas, cobra especial importancia el concepto de confiabilidad, es
decir, garantizar el correcto funcionamiento del sistema en todo momento. En la
figura \ref{fig:01-dependability} se pueden apreciar los atributos que debe
poseer un sistema confiable. A nivel de software, existen arquitecturas y
patrones de diseño orientados a garantizar la tolerancia ante los fallos del
mismo \cite{pullum_software_2001}\cite{randell_system_1975}. Por otra parte, la
replicación de componentes \cite{hutchison_dependable_2006} es una técnica muy
usada tanto para el software como el hardware. Lo que se intenta con la
replicación es asegurar que un cálculo o procesamiento se lleva a cabo de forma
correcta, aunque alguna de las réplicas falle. En \cite{amin_review_2019}, se
realiza una revisión de los sistemas de control tolerantes ante fallos
dividiéndolos en tres tipos: AFTCS (\textit{Active Fault Tolerant Control
  Systems}), PFTCS (\textit{Passive Fault Tolerant Control Systems}) o HFTCS
(\textit{Hybrid Fault Tolerant Control Systems}). Para cada uno de estos tipos,
los autores presentan las principales arquitecturas usadas, los modelos de
análisis matemático usados para su validación y las últimas técnicas usadas para
su diseño. Por otra parte, el estudio realizado en \cite{shan_survey_2019} tiene
como objetivo analizar la aplicación de los estándares de seguridad, protección
y privacidad al diseño y desarrollo de sistemas confiables, llegando los autores
a la conclusión de que cada vez están ganando más popularidad los de protección
y privacidad, aunque los procesos para asegurar estos aspectos son menos maduros
que los relacionados con la seguridad. Además, también se identifica una falta
de acción combinada en estos aspectos, trabajando normalmente por separado en
cada uno de ellos.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{01-introduction/dependability.png}
  \caption{Principales aspectos de la confiabilidad}
  \label{fig:01-dependability}
\end{figure}

Para muchos sistemas de control, el tiempo es también un aspecto relevante a la
hora de garantizar su correcto funcionamiento. Estos son los llamados sistemas
de tiempo real. En un sistema normal, en el que se obtiene una salida al aplicar
una cierta lógica sobre la entrada, la corrección de dicha salida depende de la
corrección de la lógica aplicada, es decir, del cálculo realizado. No obstante,
cuando hablamos de sistemas de tiempo real, la corrección de la lógica no es
suficiente para determinar que el funcionamiento es correcto, sino que esta
corrección depende también del momento temporal en el que se obtiene
\cite{gambier_real-time_2004}. En otras palabras, si el cálculo es correcto pero
el resultado no llega en el momento en el que se necesita, se considera como un
fallo del sistema. Las tareas que ejecuta un sistema de tiempo real están, por
tanto, sujetas a restricciones temporales. Normalmente, hablamos de un tiempo
límite o \textit{deadline} antes del cuál se debe haber completado la tarea.
Comúnmente, se suelen clasificar las tareas en dos tipos dependiendo de las
consecuencias de incumplir con sus restricciones temporales:

\begin{itemize}
  \item Tareas de tiempo real «blandas»: Se denominan así aquellas tareas en las
        que no completar las mismas antes del límite supone una reducción en la
        calidad del servicio (QoS).
  \item Tareas de tiempo real «duras»: En estas tareas, el no cumplir con las
        restricciones temporales supone un fallo grave del sistema con posibles
        consecuencias catastróficas.
\end{itemize}

Para implementar estos sistemas, se hace uso de herramientas y algoritmos de
planificación de procesos específicos que permiten la ejecución de estos tipos
de tareas cumpliendo con sus restricciones temporales. Esto se explica en más
detalle en la sección \ref{sec:real-time}.

En este trabajo, nos centraremos especialmente en los sistemas de control
aplicados a entornos industriales. Debemos diferenciar entre dos conceptos: PCS
(\textit{Process Control System}), que es el sistema encargado de controlar una
parte concreta de la planta (p. ej., una turbina o un brazo robótico), e ICS
(\textit{Industrial Control System}), que se refiere al control de toda la
planta al completo. Un ICS está compuesto, por tanto, de múltiples PCS que se
encargan de controlar los distintos aspectos del proceso de producción. Estos
PCSs son sistemas empotrados como los que hemos comentado a los que se aplican
también los conceptos de confiabilidad y seguridad. En
\cite{krotofil_industrial_2013} se realiza un estudio del estado del arte en
cuanto a la seguridad de los ICS, concluyendo que, aunque las vulnerabilidades
de muchos de los sistemas actuales son conocidas, la aplicación de parches que
las solucionen son inviables debido a los altos costes asociados a la
recertificación o directamente debido a la incompatibilidad con algunos sistemas
más antiguos. Es necesario, por tanto, incorporar los conceptos de seguridad a
los procesos de diseño desde el primer momento para evitar dar lugar a estas
situaciones.

En 2007, Vestal \cite{vestal_preemptive_2007} publica una primera propuesta para
la planificación de conjuntos de tareas de criticalidad mixta. Este importante
avance es considerado por muchos como el inicio de la investigación en sistemas
de criticalidad mixta o MCS (\textit{Mixed-Criticality Systems}). La idea detrás
de estos sistemas es, como su propio nombre indica, poder ejecutar sobre una
misma plataforma hardware tanto tareas críticas como tareas no críticas o con
niveles de criticalidad menores, asegurando en todo momento que las
restricciones temporales se cumplen, al menos para las tareas más críticas.
Desde entonces, se han planteado muchos modelos para la implementación de estos
sistemas. En \cite{burns_mixed_2015}, Burns realiza una revisión de toda la
investigación realizada en este campo hasta marzo de 2019. Se muestran en esta
revisión algunas arquitecturas propuestas, además de las principales técnicas de
análisis para estos sistemas en plataformas uniprocesador y multiprocesador.
Burns identifica la conciliación entre la separación de los procesos y la
compartición de los recursos como el principal problema de los MCS. En este
aspecto, nuestro trabajo propone el uso de contenedores como medio de ejecución
de los distintos procesos sobre una misma plataforma consiguiendo esa
separación.

No podemos terminar nuestra revisión de los sistemas empotrados sin hablar sobre
el uso del kernel de Linux para su implementación. Como referencia, hemos tomado
dos estudios realizados en 2004 sobre el uso de Linux para sistemas empotrados
\cite{geer_survey_2004}\cite{henkel_munichmit_2004}. En el primero, se destaca
la buena situación de Linux en este campo, suponiendo las soluciones comerciales
basadas en el kernel de Linus Torvalds el 15,5\% del mercado. El segundo estudio
es una encuesta realizada a 268 personas que trabajan en el campo de los
sistemas empotrados, ya sea académicamente o de forma comercial. Lo más
destacable es que la mayoría de los participantes usan Linux en sistemas
empotrados para comunicaciones, dispositivos móviles o control de maquinaria.
Esto último es especialmente alentador para este trabajo, en el que pretendemos
usar Linux como base para la implementación de ICS.

\section{La nube en los sistemas industriales: fog/edge computing para sistemas
  de tiempo real}

Desde su aparición alrededor de 2006, la computación en la nube o \textit{cloud
  computing} ha experimentado un crecimiento abismal, cambiando completamente la
manera en la que las organizaciones gestionan su infraestructura y en la que los
usuarios acceden a servicios. Atendiendo a la definición del NIST
\cite{mell_nist_2011}, se trata de un modelo de prestación de servicios que
ofrece acceso ubicuo, prácticamente ilimitado y bajo demanda a un conjunto de
recursos de computación (p. ej., almacenamiento, tiempo de procesamiento,
comunicaciones), todo ello a través de internet. Sus principales características
son:

\begin{itemize}
  \item Acceso bajo demanda: El consumidor es el que decide qué recursos
        necesita y en qué cantidad, sin necesidad de intervención humana por parte del
        proveedor del servicio.
  \item Consumo a través de internet: El acceso a estos recursos se realiza
        mediante los protocolos ya conocidos y bien establecidos que potencian la web
        (p. ej., HTTP), permitiendo así un acceso ubicuo.
  \item Acceso compartido: Los recursos del proveedor se uniformizan, formando
        una especie de bolsa de recursos que es usada por múltiples consumidores
        (\textit{multi-tenant}). Por ejemplo, aunque desde el punto de vista de un
        usuario pueda parecer que tiene una CPU para él solo, en realidad la CPU
        física es usada por varios usuarios a la vez.
  \item Elasticidad: Dependiendo de la demanda, se pueden asignar o liberar los
        recursos de forma dinámica, consiguiente sistemas más eficientes y capaces de
        dar respuesta a cargas de trabajo más altas.
  \item Servicio medido: El uso de los recursos se monitoriza de forma
        automática, lo que proporciona transparencia tanto para el consumidor como el
        proveedor.
\end{itemize}

A raíz de la propuesta de la computación en la nube, han surgido varios modelos
de prestación de servicios a usuarios. Según el tipo de recurso que ofrecen,
estos modelos se pueden clasificar principalmente en SaaS (\textit{Software as
  a Service}), PaaS (\textit{Platform as a Service}) o IaaS
(\textit{Infrastructure as a Service}). Desde el punto de vista de los usuarios,
estos modelos tienen el beneficio de que permiten pagar solo por lo que
necesitas en cada momento, adaptándose perfectamente a tus necesidades. Además,
estos usuarios ya no se ven limitados por el hardware que poseen. Prácticamente
cualquier ordenador es capaz de conectarse a internet y, de esta forma, acceder
a unos recursos prácticamente ilimitados en la nube. Esto es especialmente
evidente para las empresas, las cuáles ya no necesitan mantener su propia
infraestructura de hardware para soportar su operación diaria, con los enormes
costes y complicaciones que esto supone.

En el campo del control industrial, en el que se centra este trabajo, podríamos
pensar en la nube como una plataforma ideal para procesar las enormes cantidades
de datos que producen las plantas e implementar las técnicas de aprendizaje
automático necesarias para mejorar su autonomía. No obstante, existe un gran
problema: la latencia. En el modelo de la nube, todo el procesamiento está
centralizado en un centro de datos\footnote{Normalmente, existen varios centros
  de datos distribuidos geográficamente, pero a efectos del problema que se
  plantea, se sigue viendo como una «centralización».}, el cuál se encuentra lejos
de la planta donde se recogen los datos. Enviar estos datos a la nube y esperar
una respuesta supone demasiado tiempo como para poder considerar delegar en ella
tareas de control de la operación de la planta. En entornos controlados, donde
el centro de datos se encuentra relativamente cerca de la planta, las pruebas
llevadas a cabo en \cite{hofer_industrial_2019} indican que la latencia puede
ser lo suficientemente predecible para la implementación de ciertos sistemas de
control con requisitos temporales suaves. Sin embargo, se trata de una situación
poco factible, ya que es inviable ubicar un centro de datos cerca de todas las
instalaciones industriales. En \cite{piggin_are_2015}, se analiza si los
ICS están preparados para ser desplazados a la nube, pero centrándose más en el
aspecto de la seguridad de estos sistemas. El autor identifica que, aunque la
nube es una plataforma muy robusta, también son flagrantes las dudas que produce
en cuanto a su seguridad frente a ataques. Por estas razones, no consideramos
que la nube sea un paradigma adecuado para dar respuesta a los nuevos requisitos
de la industria.

Aunque la nube no sea viable, bien es cierto que las ideas de elasticidad y
ubicuidad que plantea son interesantes para el problema en cuestión. Al final
del día, si queremos realizar análisis en tiempo real de los datos que generan
los procesos industriales para tomar decisiones, necesitamos más potencia
computacional de la que ningún dispositivo individual nos puede ofrecer. ¿La
solución? Aprovechar los recursos de computación de todos los dispositivos ya
presentes en las plantas industriales. De acercar estas ideas de la nube a los
dispositivos del borde de la red surgen dos nuevos paradigmas de computación:
\textit{fog} y \textit{edge computing}. En estos paradigmas, se pretende acercar
el procesamiento que se realiza sobre los datos a los propios dispositivos que
generan dichos datos. En el contexto industrial, esto también supone estar más
cerca de los dispositivos que deben actuar en respuesta a este procesamiento,
con lo que se obtendrían tiempos de respuesta mejores que con la nube. La
diferencia entre \textit{fog} y \textit{edge} radica en lo cerca del borde de la
red que ubiquemos el procesamiento de los datos, si bien es cierto que en
algunos escritos se usan los términos \textit{fog} y \textit{edge} de manera
intercambiable \cite{shi_edge_2019}, sin diferencia aparente entre ambos.

\begin{itemize}
  \item En \textit{fog}, se trata de nodos ubicados en la misma red local que
        las fuentes de datos.
  \item En \textit{edge}, los propios dispositivos que generan los datos ya
        realizan un cierto procesamiento de los mismos.
\end{itemize}

En la figura \ref{fig:02-cloud_fog_edge} se puede apreciar como estos dos
paradigmas, junto con la nube, constituyen una arquitectura por capas, donde
cada uno de ellos tiene unas características concretas. Se trata, por tanto, de
modelos complementarios, no exclusivos. En la capa \textit{edge}, donde se
generan los datos, se pueden implementar los mecanismos de control con
restricciones más duras, ya que se tienen unos tiempos de respuesta más rápidos
al actuar directamente en base a los datos generados. En la capa \textit{fog},
los nodos pueden realizar tareas de análisis de datos más exigentes, dado que
poseen más capacidad de computación que los dispositivos del \textit{edge}.
También se pueden implementar aquí tareas de control. Por último, la nube
recibiría todos los datos producidos por la planta, además de otras plantas que
también posea la organización, realizando la agregación y el análisis en
profundidad de los datos, obteniendo informes que puedan ayudar a los
supervisores en la identificación de fallos y la mejora del rendimiento.

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{02-state_of_the_art/cloud-fog-edge.png}
  \caption{Estructura por capas \textit{cloud-fog-edge}}
  \label{fig:02-cloud_fog_edge}
\end{figure}

Dejando atrás la ya explicada nube, vamos a adentrarnos más en el \textit{fog}.
Esta capa se compone de los llamados nodos \textit{fog}, que son los encargados
de llevar a cabo las distintas tareas. Cualquier dispositivo con capacidad de
procesamiento, memoria y conexión de red puede ser un nodo viable. De esta
forma, se pueden usar como nodos routers, switches y otros dispositivos de
redes, así como cámaras de videovigilancia. Se aprovecha la capacidad de
procesamiento de todos estos dispositivos cuando no se están usando al 100\%.
Como ya se ha indicado, estos nodos se encuentran ubicados en la misma red local
(LAN) que los dispositivos del borde de la red que generan los datos. Las
distintas tareas de análisis y control a realizar sobre estos datos se
distribuyen sobre los nodos \textit{fog} de forma dinámica dependiendo de la
carga de trabajo que posean los mismos. A la hora de asignar una tarea a un
nodo, siempre se debe intentar hacer de forma que el nodo elegido esté lo más
cerca posible de la fuente de los datos \cite{gedeon_fog_2018}. Esto es
especialmente importante para tareas de control, donde es posible que no todos
los nodos \textit{fog} de la planta tengan la latencia necesaria para cumplir
con los requisitos temporales. En este sentido, en \cite{pop_enabling_2018} se
propone el uso de TSN (\textit{Time-Sensitive Networking}) para las
comunicaciones en la capa \textit{fog}, con el objetivo de reducir estas
latencias. Este modelo de computación distribuida sobre una bolsa o
\textit{pool} de recursos es muy similar al que encontramos en la nube, con la
diferencia de que en el \textit{fog} estos recursos son mucho más limitados, por
lo que es también esencial optimizar el uso de los mismos. Debido a esta
similitud con el modelo de la nube, las mismas tecnologías de virtualización
usadas en ésta también son útiles en el nuevo paradigma \cite{yi_fog_2015}.
Algunos modelos ya han sido propuestos para la aplicación del \textit{fog} a los
sistemas industriales y el modelado de la Industria 4.0
\cite{verba_modeling_2019} \cite{tseng_lightweight_2018}.

Aunque la capacidad de procesamiento del \textit{fog} es mucho menor que la de
la nube, sigue siendo útil para realizar algunas tareas menos intensivas
evitando tener que desplazar todos los datos a la nube, con la latencia y la
carga en la red que eso supone. Como consecuencia del avance del IoT
(\textit{Internet of Things}) industrial, que es considerada una de las
tecnologías más importantes para la Industria 4.0 \cite{lu_industry_2017}, el
número de dispositivos presentes en las plantas y, por tanto, la cantidad de
datos que se generan va a ser mucho mayor, poniendo aún más presión sobre la
red, que es donde se podría producir el cuello de botella \cite{shi_edge_2016}.
El \textit{edge} trata de solucionar esto al reducir la cantidad de datos que se
deben enviar hacia las capas superiores procesando una parte de ellos de forma
local. Cabe destacar que el \textit{fog} también permite relajar el tráfico de
red \cite{wang_traffic_2019}. Los dispositivos del borde de la red, aún
ofreciendo menos potencia que la combinada del \textit{fog}, también pueden ser
usados para realizar algunas acciones sobre los datos. En concreto, las tareas
de control que requieran de un tiempo de respuesta más rápido pueden realizarse
en esta capa, tomando decisiones directamente sobre los datos que recoge el
dispositivo. Por otra parte, Khan \cite{khan_edge_2019} identifica la falta de
confianza en los sistemas \textit{edge} como uno de los principales problemas a
los que se enfrenta el paradigma, además de la integración de sistemas y la
movilidad. Esta falta de confianza es especialmente grave si pretendemos
desplegar sistemas de control críticos sobre plataformas de este tipo. Como
solución al problema de la confianza, Stanciu \cite{stanciu_blockchain_2017}
propone usar tecnología de \textit{blockchain} para desplegar sistemas de
control distribuidos en el \textit{edge}.

Sectores como el automovilístico, energético o de alimentación están muy
interesados en el desarrollo de la Industria 4.0 debido a los importantes
beneficios que se estima que puede tener para sus procesos. Gracias al 5G y a
las mejoras en las comunicaciones, el IoT se convierte en una idea capaz de dar
a las organizaciones una cantidad de información sobre sus procesos industriales
jamás vista antes. Estos datos son la base de la Industria 4.0
\cite{khan_perspective_2016}, ya que permiten la definición de sistemas
ciber-físicos y gemelos digitales que replican perfectamente los procesos
físicos en el plano digital, lo que abre la puerta a un control inédito sobre
dichos procesos. Además de las comunicaciones, otro reto muy importante para la
consecución de estos objetivos es el modelado de estos sistemas cuidando los
aspectos deseados de seguridad y fiabilidad \cite{farsi_industry_2019}. En este
sentido, creemos que el \textit{fog} es un modelo de computación que puede
facilitar el diseño de estos CPS al proporcionar una plataforma robusta y
unificada sobre la que implementarlos.

\section{Tecnologías de virtualización: hipervisores y contenedores}

De manera simple, la virtualización consiste en abstraer ciertas capas de un
sistema convencional (p. ej., el hardware). Sobre estas abstracciones se pueden
ejecutar procesos o sistemas completos de manera aislada. Este aspecto es el que
hace de la virtualización un concepto tan relevante para la computación en la
nube, ya que posibilita el acceso compartido a los recursos. Por ejemplo, sobre
una misma CPU se podrían ejecutar varios sistemas operativos completamente
funcionales de forma que ninguno de ellos sepa de la existencia de los otros.
Cada uno de estos sistemas cree que tiene una CPU completa para él gracias a la
abstracción del hardware. Aunque las tecnologías de virtualización han
experimentado un empuje enorme en los últimos años debido a la expansión de la
nube, no se trata de una idea novedosa. En 1974, Popek y Goldberg
\cite{popek_formal_1974} definían los requisitos que debía cumplir una
plataforma para poder virtualizar sistemas completos. Hoy en día, hablamos de
virtualización en dos niveles diferentes: a nivel del hardware (máquinas
virtuales) o a nivel del sistema operativo (contenedores).

Lo que se quiere decir con virtualización a nivel del hardware, es que es éste
el que se abstrae. Esto es precisamente lo que sucede con las máquinas
virtuales. El hardware «real» se puede fraccionar para asignarlo a las diversas
máquinas virtuales, que lo usan como si se tratase de una plataforma física
normal y corriente. Así, se puede regular la cantidad de CPU, memoria RAM o,
incluso, acceso a dispositivos de I/O que tiene cada máquina virtual. Todo esto
es posible gracias a los hipervisores, que son la pieza de software encargada de
abstraer el hardware y regular su uso por parte de las máquinas virtuales.
Dependiendo de la relación existente entre el hipervisor y el hardware que
abstraen, nos encontramos con hipervisores de tipo 1 y 2. Un hipervisor de tipo
2 se ejecuta sobre un sistema operativo tradicional, que recibe el nombre de
anfitrión, mientras que los de tipo 1 prescinden del anfitrión, ejecutándose
directamente sobre el hardware que gestionan (\textit{bare metal}). En la figura
\ref{fig:02-hypervisors} se representa gráficamente esta diferencia.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{02-state_of_the_art/hypervisors.png}
  \caption{Capas de la virtualización mediante hipervisores de tipo 1 y 2}
  \label{fig:02-hypervisors}
\end{figure}

Como es obvio, los hipervisores de tipo 1 suponen una solución mucho más liviana
que los de tipo 2 al no tener un anfitrión. Por otra parte, pueden asignar
prácticamente todos los recursos hardware de la plataforma a las máquinas
virtuales de forma dinámica, mientras que los de tipo 2 se ven restringidos a
los límites impuestos por el anfitrión. Los hipervisores de tipo 2 son más
utilizados en el ámbito personal, con herramientas como VirtualBox o VMWare
siendo muy utilizadas para la prueba de software, mientras que los de tipo 1 son
los que predominan a nivel empresarial e industrial, con claros ejemplos como
PikeOS o Xen. Este último supuso un importante avance en la ejecución segura y
aisla de máquinas virtuales sobre hardware de uso común \cite{barham_xen_2003}.
En lo que a sistemas de tiempo real se refiere, RT-Xen \cite{xi_rt-xen_2011} se
presentó como un marco de referencia para la planificación de procesos con
restricciones temporales en sistemas Linux virtualizados usando Xen, obteniendo
prometedores resultados.

Mientras que en la virtualización a nivel del hardware las máquinas virtuales
contienen un sistema operativo completo, en la virtualización a nivel del
software solo se virtualizan procesos. Estos procesos virtualizados, que reciben
comúnmente el nombre de contenedores, acceden a los servicios básicos del
sistema operativo anfitrión. De forma análoga a los hipervisores, los motores de
contenedores son los encargados de gestionar los procesos y su acceso al sistema
operativo que comparten. Realmente, los procesos contenerizados se ejecutan
sobre el sistema anfitrión como lo harían los procesos «nativos». La única
diferencia se encuentra en que los procesos contenerizados están aislados del
resto. Estos procesos solo pueden ver al resto de procesos de su mismo
contenedor, no los del resto de contenedores ni los no contenerizados. Para
conseguir este aislamiento, los motores de contenedores aprovechan varias
características del kernel de Linux, entre las que destacamos:

\begin{itemize}
  \item Espacios de nombres (\textit{namespaces}): los identificadores de
        proceso (PID), las interfaces de red o la parte del sistema de ficheros que
        puede ver un proceso contenerizado depende del espacio de nombres al que se
        asigne cuando se crea. Usando esta herramienta, se puede restringir lo que ven
        los contenedores del sistema anfitrión.
  \item Grupos de control (\textit{control groups}): Los \texttt{cgroups} son
        mecanismos que permiten regular el acceso al hardware por parte de los
        contenedores, así como su acceso compartido.
\end{itemize}

A primera vista, cabría pensar que, dado que contienen un sistema operativo
completo, las máquinas virtuales son mucho más pesadas que los contenedores. No
obstante, en trabajos como \cite{manco_my_2017} \cite{felter_updated_2015} se
desmiente esta percepción. Concretamente, en el último se compara el popular
motor de contenedores Docker con KVM, un famoso hipervisor para Linux, llegando
a la conclusión de que ninguno de los dos penaliza de manera significativa el
rendimiento en cuanto a uso de CPU o memoria, si bien es cierto que las máquinas
virtuales de KVM obtenían peores resultados en las operaciones de I/O. Por otro
lado, la seguridad en los contenedores es un aspecto todavía en desarrollo
\cite{randal_ideal_2020}, sobre todo en la seguridad del kernel compartido.

Existen múltiples motores de contenedores usados ampliamente en la actualidad,
como es el caso de Docker, Rocket (rkt) o LXC (\textit{LinuX Containers}).
Kozhirbayev y Sinnot \cite{kozhirbayev_performance_2017} realizaron en 2017 una
comparativa del rendimiento entre Docker y LXC\footnote{Concretamente, se hace
  uso de Flockport.}. Los resultados obtenidos coinciden con los vistos en
\cite{felter_updated_2015} en tanto que los procesos contenerizados no tienen
una penalización en el consumo de CPU o RAM frente a los nativos, aunque sí que
se observa esta penalización en las operaciones de I/O. Como el rendimiento es
similar para todos los motores de contendores, se ha decidido poner el foco
sobre Docker para este trabajo debido a que es el más popular, con el
consecuente impacto que esto tiene en la calidad del soporte y de la
documentación disponible.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{02-state_of_the_art/docker.png}
  \caption{Esquema de funcionamiento de Docker}
  \label{fig:02-docker}
\end{figure}

En Docker, al igual que en muchos otros motores de contenedores, los
contenedores se lanzan a partir de imágenes, definidas mediante un
\texttt{Dockerfile}. Una imagen es reutilizable y universal, pudiendo lanzar
réplicas de un mismo contenedor en cualquier plataforma soportada por la imagen.
Para trabajar con imágenes y contenedores, el sistema Docker se compone de dos
partes principales: el demonio\footnote{Un demonio o \textit{daemon} es un tipo
  de proceso que se ejecuta en segundo plano para ofrecer un servicio.} y el
cliente. El demonio es el encargado de realizar todas las soperacones
relacionadas con imágenes o contenedores, incluyendo su construcción,
lanzamiento y gestión posterior. Para realizar estas acciones, expone una API
con la que pueden interactuar los distintos clientes, siendo el oficial que
proporciona Docker una aplicación de línea de comandos. A estos dos componentes
se podría añadir un tercero, el registro de imágenes. Como su propio nombre
indica, se trata de una especie de base de datos de imágenes listas para su uso.
De esta forma, se pueden almacenar y compartir imágenes entre varios
dispositivos a través de la red. No obstante, se puede hacer uso de Docker de
manera local sin necesidad de usar un registro externo. Un esquema del
funcionamiento de Docker que se acaba de explicar se puede ver en la figura
\ref{fig:02-docker}.

Centrando nuestra atención de nuevo en los sistemas de control industrial, se
plantea la posibilidad de implementarlos usando contenedores
\cite{hofer_industrial_2019}, aunque los principales inconvenientes
identificados son los relacionados con la seguridad. En
\cite{cinque_rt-cases_2019}, los autores proponen un modelo de planificación de
procesos contenerizados para conjuntos de tareas con distintos niveles de
criticalidad, defendiendo el uso de contenedores por su capacidad de
aislamiento. Sin embargo, en la encuesta sobre contenerización para tiempo real
realizada por Struhár et al. en 2020 \cite{struhar_real-time_2020}, se llega a
la conclusión de que son necesarias mejores herramientas para la gestión de
contenedores de tiempo real. Por ello, en este trabajo se plantea el desarrollo
de una herramienta para este fin como prueba de concepto. Los autores del
estudio destacan también la falta de un mecanismo determinista de comunicación
entre procesos contenerizados y de herramientas de validación y prueba, además
de los problemas de seguridad ya mencionados.

\section{Aspectos de computación en tiempo real: RTOS, algoritmos de
  planificación y herramientas}
\label{sec:real-time}

En esta sección, se va a explicar con mayor detalle el funcionamiento de los
sistemas de tiempo real ya mencionados en las secciones anteriores. En estos
sistemas, el tiempo es un factor crucial a la hora de determinar la validez de
las salidas. De esta forma, las tareas ejecutadas se ven limitadas por
restricciones temporales. Dependiendo de la severidad de las consecuencias del
incumplimiento de las restricciones, solemos hablar de tareas y restricciones
duras o blandas. Las restricciones duras son aquellas que, si se incumplen,
tienen consecuencias son muy graves, como pueden ser daños a la vida de las
personas en el caso de sistemas que sean críticos. En cuanto a las restricciones
blandas, solo se experimenta una pérdida de calidad en el servicio cuando se
incumplen, aunque se suele limitar el número de incumplimientos permitido. En la
figura \ref{fig:02-deadlines} se muestra la diferencia entre los dos tipos de
restricciones según su función de utilidad \cite{wang_introduction_2013}, que es
la que define el valor de la salida del sistema en función del tiempo. Como se
puede apreciar, el resultado de una tarea dura solo es útil si se da entre el
inicio de la tarea y su límite temporal fijado, provocando daños de cualquier
otra manera. En las tareas blandas, el resultador puede tener valor incluso si
se obtiene después del límite, aunque se va reduciendo con el tiempo.

Debido a la gran responsabilidad que recae sobre el cumplimiento de estos
requisitos, los sistemas empotrados sobre los que se ejecutan estas tareas
tienen características diferentes a las de los sistemas de uso general. A nivel
del hardware, por ejemplo, las CPU tienen un diseño más simple que haga más
fácil verificar el determinismo en sus operaciones. En
\cite{rotenberg_chapter_2005}, Rotenberg y Anantaraman describen el
funcionamiento de las CPU de alto rendimiento y las comparan con las empotradas.
La ejecución predictiva, por ejemplo, es severamente limitada en los sistemas
empotrados. Por otro lado, las operaciones de acceso a caché y RAM suponen están
optimizadas para necesitar un menor número de ciclos. Normalmente, el conjunto
de instrucciones (ISA) de estas CPU es reducido, conocido como RISC, reduciendo
la complejidad y facilitando la programación a bajo nivel. ARM es una de las
arquitecturas de microprocesador basadas en RISC más usadas para el diseño de
sistemas empotrados. Otro aspecto diferencial es el número de núcleos o
\textit{cores} presentes en los procesadores. Poseer varios núcleos hace posible
la ejecución en paralelo de instrucciones, mejorando el rendimiento en los
sistemas. No obstante, este paralelismo dificulta asegurar el determinismo,
aunque algunos algoritmos de planificación de procesos han sido propuestos
\cite{anderson_edf-based_2005} y la investigación es activa en este área. Para
asegurar además que las tareas se puedan ejecutar correctamente, los sistemas de
tiempo real suelen dejar capacidad de CPU extra libre.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{02-state_of_the_art/deadlines.png}
  \caption{Funciones de utilidad para restricciones temporales duras y blandas}
  \label{fig:02-deadlines}
\end{figure}

Además del hardware, el software usado para implementar estos sistemas también
es especial. Los lenguajes y herramientas deben proporcionar al programador
ciertas comodidades para la implementación de procesos en tiempo real
\cite{burns_real-time_2009}:

\begin{itemize}
  \item Especificar momentos en los que las acciones se deben realizar.
  \item Especificar los momentos antes de los cuales las acciones deben terminar.
  \item Permitir la ejecución repetitiva (periódica y aperiódica) de trabajo.
  \item Limitar la varianza en las operaciones de entrada y salida.
  \item Responder a situaciones donde no todas las restricciones temporales se
        pueden cumplir.
  \item Responder a situaciones en las que los requisitos de tiempo cambian de
        manera dinámica.
\end{itemize}

Una parte muy importante de los sistemas empotrados es su sistema operativo, el
cual gestiona los recursos físicos de la plataforma y controla el uso de los
mismos por parte de los procesos implementados por los desarrolladores. Al igual
que ocurre con el hardware, estos sistemas operativos también tienen
características diferentes a los de propósito general para poder dar cobijo a
tareas con restricciones temporales, recibiendo el nombre de RTOS (\textit{Real
  Time Operating System}). La principal diferencia, podríamos decir que se
encuentra en los algoritmos de planificación de tareas usados, aunque cabe
destacar que también hay otras áreas importantes como el acceso a memoria o la
gestión de las interrupciones. El algoritmo de planificación es, a grandes
rasgos, la política que decide en qué orden se ejecutan las tareas del sistema.
En los sistemas normales, normalmente se asigna a cada proceso una fracción
igualitaria de tiempo de CPU, ejecutándose hasta que consume su fracción de
tiempo o se bloquea (p. ej., por una operación de lectura o escritura). En los
sistemas de tiempo real, este método no es factible, ya que los procesos tienen
distintas prioridades y no pueden quedar a la espera de que otro proceso de
menor prioridad libere la CPU. Por ello, la planificación de procesos de tiempo
real es apropiativa (\textit{preemptive scheduling}), de forma que el sistema
operativo puede quitar la CPU a un proceso que está en ejecución para
asignársela a otro más prioritario. Como se destaca en
\cite{burns_scheduling_1991}, la planificación de procesos óptima para un
sistema de tiempo real «duro» depende enormemente del problema en cuestión. ¿Se
trata de un sistema uniprocesador o multiprocesador? ¿Todas las situaciones son
conocidas de antemano? ¿Las tareas son periódicas o aperiódicas? Dependiendo de
la situación, podemos encontrarnos con que un algoritmo de planificación es más
eficaz que otro.

Antes de entrar a explicar algunos de los algoritmos de planificación de tareas
más importantes, debemos entender mejor como son estas tareas. Las tareas en un
sistema de tiempo real pueden ser provocadas por tiempo o por eventos. Las
tareas provocadas por tiempo son tareas periódicas, con un tiempo definido para
sus ciclos. Las caracterizamos con los siguientes atributos:

\begin{itemize}
  \item Tiempo de ejecución $WCET$ (\textit{Worst Case Execution Time}): Se trata
        del tiempo que se necesita para completar la tarea en el peor caso.
  \item Límite de tiempo $D$ (\textit{Deadline}): Momento temporal posterior a
        la activación de la tarea y antes del cuál se debe haber completado.
  \item Período $T$ (\textit{Period}): Es el tiempo que transcurre entre
        activaciones.
\end{itemize}

Estas tareas son ejecutadas por un reloj interno del sistema, el cuál debe ser
lo suficientemente preciso como para asegurar que los ciclos se cumplen. Por
otra parte, las tareas también pueden ser aperiódicas si son activadas por un
evento externo (p. ej., un cambio en el entorno, como sucede en algunos sistemas
de control) mediante mecanismos como las interrupciones. Las tareas aperiódicas
siguen teniendo $WCET$ y $D$, pero no tienen período $T$. En algunos casos,
existe un tiempo mínimo entre activaciones $T$, considerándose la tarea como
esporádica. En ocasiones, los sistemas empotrados limitan su arquitectura para
que solo haya tareas activadas por tiempo, evitando las interacciones y
comprobando de forma periódica si se ha producido un evento. De todos estos
atributos, el tiempo de ejecución en el peor caso es el más difícil de
determinar. Se trata de un límite superior que puede ser estimado usando métodos
estáticos, probabilísticos o, incluso, basados en mediciones. No obstante, la
fiabilidad de muchos de estos métodos se apoya en supuestos sobre el sistema y
su funcionamiento \cite{abella_wcet_2015}.

Para evaluar la efectividad de un algoritmo de planificación, solemos basarnos
en su capacidad para cumplir con todas las restricciones temporales de un
conjunto de tareas dado. Cuando se consiguen respetar todas estas restricciones,
se dice que la planificación (o el conjunto de tareas) es viable, mientras que
si el sistema se salta alguna, se suele decir que está sobrecargado. Un concepto
muy importante en el que se apoyan los algoritmos de planificación para
determinar la viabilidad de un conjunto de tareas es la utilización total de CPU
para dicho conjunto, definida como

\begin{equation}
  U = \sum_{i=1}^{n} \frac{C_{i}}{min(D_{i}, T_{i})}
\end{equation}

por Liu y Layland \cite{liu_scheduling_1973}, donde $C_{i}$, $D_{i}$ y $T_{i}$
son el tiempo de ejecución, límite temporal y período para la tarea $i$ del
conjunto, respectivamente.

Anteriormente, se ha indicado que la planificación de procesos de tiempo real es
apropiativa, de forma que el sistema operativo puede retirarle a un proceso los
recursos que tenía asignado para dárselos a otro de mayor prioridad. La
asignación de estas prioridades a las distintas tareas es la base de los
algoritmos de planificación. Estos algoritmos pueden ser de dos tipos: estáticos
o dinámicos. Los algoritmos estáticos realizan la planificación y asignan las
prioridades a las tareas antes incluso de tener que poner en marcha el sistema,
de forma \textit{offline}, ejecutándose luego las tareas en el orden determinado
por las prioridades establecidas. También conocida como planificación de
prioridades fijas FPS (\textit{Fixed-Priority Scheduling})
\cite{tilborg_fixed_1991}, para aplicar estos algoritmos es necesario conocer
todas las características del problema de planificación (número de tareas y sus
atributos) de antemano, lo cuál es imposible en algunas situaciones en las que
se desconoce la carga de trabajo futura del sistema. Uno de los algoritmos
estáticos más conocidos es RMS (\textit{Rate Monotonic Scheduling})
\cite{liu_scheduling_1973}\cite{kao_rate-monotonic_2008}, cuya idea principal es
que las tareas con los períodos más pequeños tienen mayor prioridad. En la
propuesta original de RMS realizada por Liu y Layland, asumen un conjunto de
tareas periódicas con $D=T$ y determinan el límite superior para la utilización
como

\begin{equation}
  U \leq n(2^{\frac{1}{n}} - 1)
\end{equation}

Esto implica que, para conjuntos de tareas muy grandes ($n \rightarrow \infty$),
la utilización de CPU tiene que ser inferior al 70\%, que es una cifra
relativamente baja. No obstante, si se cumplen todas estas condiciones, RMS
garantiza encontrar una planificación viable si es que existe, o lo que es lo
mismo, es un algoritmo óptimo. Otro problema de RMS es que requiere que el
período sea igual al límite de tiempo para cada tarea. Para solucionar este
problema, se propone DMT (\textit{Deadline Monotonic Scheduling})
\cite{audsley_hard_1991}, en el que los procesos se caracterizan con $C_{i} \leq
  D_{i} \leq T_{i}$.

Como ya se ha comentado, el principal problema de la planificación estática es
que se trata de algoritmos clarividentes, es decir, necesitan conocer toda la
información sobre el conjunto de tareas para poder determinar su viabilidad. Los
algoritmos dinámicos, por otra parte, permiten determinar esta viabilidad
en tiempo de ejecución, cambiando las prioridades de las tareas sobre la marcha
u \textit{online}. Uno de los algoritmos dinámicos más conocidos es EDF
(\textit{Earliest Deadline First}) \cite{liu_scheduling_1973}, donde las tareas
con menor tiempo restante antes de su límite temporal son las que reciben una
mayor prioridad. En este caso, el algoritmo es óptimo para conjuntos de tareas
donde se cumpla la condición \cite{zhang_schedulability_2009}

\begin{equation}
  U \leq 1
\end{equation}

No obstante, esta situación se da solo en los sistemas uniprocesador. En los
multiprocesador, determinar que la utilización es inferior al número de CPU no
es una condición suficiente para garantizar que el conjunto de tareas tenga una
planificación viable. En 2005, Anderson adapta EDF para sistemas multiprocesador
\cite{anderson_edf-based_2005}, consiguiendo una condición suficiente para
tareas de tiempo real blandas. Un enfoque diferente al de EDF lo proporciona MUF
(\textit{Maximum Urgency First}) \cite{stewart_real-time_1991}. En este
algoritmo, se trabaja en torno a la urgencia de cada tarea, que es determinada
según la combinación de varias prioridades. En concreto, cada tarea posee 2
prioridades fijas definidas antes de la ejecución y una dinámica, que se calcula
sobre la marcha, existiendo una relación de precedencia entre estas prioridades.

En el caso de ser necesario trabajar con tareas aperiódicas o esporádicas, se
puede aplicar alguno de los algoritmos propuestos en
\cite{sprunt_scheduling_1989}, entre los que destacamos DSA (\textit{Deferrable
  Server Algorithm}) y SSA (\textit{Sporadic Server Algorithm}).

Para terminar con el repaso de la computación de tiempo real, vamos a hacer un
breve repaso de algunos de los RTOS más utilizados. En
\cite{hambarde_survey_2014}, Hambarde realiza una comparación entre Windows CE,
VxWorks, QNX Neutrino y RTAI. El último, que es esencialmente una extensión del
kernel de Linux o co-kernel que da soporte a este tipo de cargas de trabajo,
obtiene los mejores resultados en latencia, \textit{jitter} y tiempo de
respuesta de entre los cuatro, si bien es cierto que su latencia ante
interrupciones es mayor que la de VxWorks. Además de RTAI, otros co-kernels para
Linux serían RTLinux y Xenomai. Todos estos co-kernels funcionan de manera
similar: implementan un despachador de interrupciones que captura las
interrupciones provenientes del hardware para redirigirlas a los procesos (tanto
normales como de tiempo real), mientras que también poseen un planificador de
procesos para asegurar que se cumplen las prioridades.

Además de los co-kernels, otra solución alternativa para Linux la encontramos en
el parche \texttt{PREEMPT\_RT}. El desarrollo de este parche es llevado a cabo
por programadores del kernel dentro de la fundación Linux y en paralelo con el
del propio kernel, lo que le da un cierto aire de «oficialidad». En esencia, los
cambios que aplica este parche sobre el kernel habilitan la planificación
apropiativa y aumentan el determinismo de ciertas operaciones. Este
acercamiento, al trabajar sobre un único kernel en vez de los dos que se
obtienen con las soluciones basadas en co-kernels, resulta en una mayor
facilidad a la hora de implementar procesos de tiempo real, ya que se realiza de
manera idéntica a los normales. Los co-kernels exponen interfaces especiales que
se salen de la interfaz estándar de Linux y con las que sus procesos deben
trabajar, algo que no ocurre con el parche \texttt{PREEMPT\_RT} y que hace que
sea muy fácil adaptar un proceso existente para que tenga un comportamiento más
determinista. Este parche habilita varios modos de planificación de procesos
nuevos para el planificador del kernel, entre los que destacaremos
\texttt{SCHED\_DEADLINE} \cite{noauthor_deadline_nodate}. Esta política de
planificación aplica el algoritmo EDF junto con CBS (\textit{Constant Bandwith
  Server}). El algoritmo CBS se encarga de asignar un ancho de banda a cada tarea,
de forma que una tarea concreta no pueda ejecutarse durante un tiempo superior a
su tiempo de ejecución dentro de cada período. De esta forma, se consigue el
aislamiento temporal de las tareas, evitando que el comportamiento de una de
ellas afecte a las demás. En cuanto a su rendimiento, el estudio llevado a cabo
en \cite{reghenzani_real-time_2019} determina que no es una solución apta para
sistemas de tiempo real duro, pero puede ser perfectamente usado cuando las
restricciones son más livianas. En cualquier caso, el rendimiento va a depender
enormemente del hardware utilizado y de la implementación de los drivers para
dicho hardware.

Para este trabajo, se ha decidido usar como plataforma objetivo
el sistema GNU/Linux parcheado con \texttt{PREEMPT\_RT} debido a que se adhiere
a la interfaz estándar del kernel y las tecnologías de virtualización mediante
contenedores presentes en la plataforma son muy maduras. El hecho de que no sea
un RTOS aceptable para sistemas con restricciones temporales duras nos es
indiferente para el alcance de este proyecto, dado que solo se busca implementar
una herramienta que sirva como prueba de concepto de las ideas planteadas.