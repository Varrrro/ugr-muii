\chapter{Diseño e implementación de un orquestador para tareas de tiempo real}

En los capítulos anteriores, se ha presentado el concepto de la Industria 4.0 y
los problemas que plantea, indicando que el paradigma del \textit{fog computing}
y las tecnologías de virtualización pueden ser una solución factible para
conseguir su implantación. Para apoyar este planteamiento, se ha decidido
implementar una herramienta de orquestación de tareas de tiempo real sobre
entornos distribuidos que sirva como prueba de concepto. En este capítulo, se
detalla el proceso seguido para su desarrollo, justificando las decisiones
de diseño tomadas y mostrando los de detalles más relevantes de la
implementación. Para terminar, se muestra una caracterización inicial del
rendimiento de la herramienta desarrollada.

\section{Objetivos y requisitos}

Como ya se ha explicado, la llegada de la Industria 4.0 supone un aumento muy
grande en la cantidad de datos generados en las plantas debido al IoT
industrial, datos que son necesarios para poder obtener gemelos digitales con un
nivel de detalle suficiente. Los nuevos sistemas ciber-físicos necesitan, por
tanto, procesar todos estos datos en tiempo real para poder tomar decisiones de
control en base a los mismos y garantizar una operación eficiente. Para ello, es
necesario poseer una capacidad de procesamiento elevada, mayor de la que
cualquier dispositivo individual pueda aportar. Aunque la computación en la nube
pueda parecer una solución natural a este problema, la enorme carga que pondría
sobre la red la constante transmisión de cantidades de datos tan grandes, así
como la latencia resultante de estas comunicaciones, hacen que no podamos
considerar esta plataforma como apropiada para dar cobijo a tareas con
restricciones temporales. En la sección \ref{sec:02-cloud_fog_edge}, se introduce
el paradigma \textit{fog} como una extensión de la nube más cercana a las
fuentes de datos. Según este modelo, los datos generados por los dispositivos
del borde de la red son procesados en nodos ubicados en la misma red local.
Aunque la potencia combinada de los dispositivos que conformen esta capa
\textit{fog} será menor que la que encontramos en la nube, puede ser suficiente
para las tareas de análisis de datos y toma de decisiones que se deben realizar
en las plantas, reduciendo así la presión sobre la red y garantizando unos
tiempos de respuesta muy inferiores.

Debido a esto, el modelo \textit{fog} se plantea como una posible plataforma
para la implementación de los nuevos sistemas ciber-físicos. Las tareas de
control industrial deben, entonces, distribuirse por los nodos de la capa
\textit{fog} de forma dinámica para dar respuesta a la carga de trabajo en todo
momento. Para que esta distribución de procesos sobre los nodos se produzca de
manera eficiente, es necesario hacer uso de las tecnologías de virtualización,
las cuales ya sirven para solucionar una problemática similar en la nube. Al
virtualizar los procesos, se abstraen las capas inferiores como son el hardware
o el sistema operativo, facilitando el despliegue de los mismos sobre
dispositivos con características dispares y unificando su desarrollo, dado que
no es necesario implementarlos para cada plataforma distinta). Así, se facilita
la escalabilidad de los procesos para dar respuesta a los cambios en la demanda.
En la sección \ref{sec:02-virtualization}, planteamos el uso de contenedores en vez
de máquinas virtuales para esto, apoyándonos en su carácter más liviano y su
mejor rendimiento para operaciones de entrada y salida.

Por tanto, en el modelo planteado es necesario desplegar las tareas de control
en forma de contenedores. Para explorar más este concepto, hemos decidido
implementar una herramienta que permita realizar esto sobre múltiples nodos. Los
objetivos de esta herramienta son:

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.7\textwidth}| }
        \hline
        Nombre      & Gestión de nodos                                       \\
        \hline
        Importancia & Alta                                                   \\
        \hline
        Descripción & Llevar un control de los dispositivos que conforman la
        capa \textit{fog} es una parte esencial de la implantación de este
        modelo.                                                              \\
        \hline
    \end{tabular}
    \caption{Objetivo 01 - Gestión de nodos.}
    \label{tab:04-obj01}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.7\textwidth}| }
        \hline
        Nombre      & Gestión de tareas                                          \\
        \hline
        Importancia & Alta                                                       \\
        \hline
        Descripción & Las distintas tareas que se deben ejecutar sobre los nodos
        de la capa \textit{fog} deben estar recopiladas y centralizadas en el
        sistema.                                                                 \\
        \hline
    \end{tabular}
    \caption{Objetivo 02 - Gestión de tareas.}
    \label{tab:04-obj02}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.7\textwidth}| }
        \hline
        Nombre      & Orquestación de tareas                                \\
        \hline
        Importancia & Muy alta                                              \\
        \hline
        Descripción & El sistema debe permitir a los usuarios desplegar las
        tareas definidas sobre los nodos de manera sencilla, controlando siempre
        que la ejecución de las tareas sobre cada nodo concreto sea viable. \\
        \hline
    \end{tabular}
    \caption{Objetivo 03 - Orquestación de tareas.}
    \label{tab:04-obj03}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.7\textwidth}| }
        \hline
        Nombre      & Uso de software libre                                      \\
        \hline
        Importancia & Media                                                      \\
        \hline
        Descripción & Como parte de nuestro compromiso con el software libre, el
        sistema desarrollador deberá hacer uso siempre que sea posible de
        tecnologías abiertas.                                                    \\
        \hline
    \end{tabular}
    \caption{Objetivo 04 - Uso de software libre.}
    \label{tab:04-obj04}
\end{table}

A partir de estos objetivos, se han concretado una serie de requisitos que
deberá satisfacer la herramienta desarrollada.

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.5\textwidth}| }
        \hline
        Nombre                 & Añadir un nuevo nodo                       \\
        \hline
        Objetivos relacionados & \ref{tab:04-obj01}                         \\
        \hline
        Descripción            & Un usuario debe poder añadir al sistema un
        nuevo nodo que represente a un dispositivo de la capa \textit{fog}. Los
        datos proporcionados para el nuevo nodo deben permitir la conexión al
        mismo por SSH.                                                      \\
        \hline
    \end{tabular}
    \caption{Requisito 01 - Añadir un nuevo nodo.}
    \label{tab:04-req01}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.5\textwidth}| }
        \hline
        Nombre                 & Obtener datos de los nodos                      \\
        \hline
        Objetivos relacionados & \ref{tab:04-obj01}                              \\
        \hline
        Descripción            & Un usuario debe poder obtener información sobre
        los nodos que hay registrados en el sistema, tanto de forma colectiva como
        detallada para un nodo concreto.                                         \\
        \hline
    \end{tabular}
    \caption{Requisito 02 - Obtener datos de los nodos.}
    \label{tab:04-req02}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.5\textwidth}| }
        \hline
        Nombre                 & Modificar un nodo                               \\
        \hline
        Objetivos relacionados & \ref{tab:04-obj01}                              \\
        \hline
        Descripción            & Un usuario debe poder actualizar la información
        relativa a un nodo ya registrado en el sistema si fuera necesario.       \\
        \hline
    \end{tabular}
    \caption{Requisito 03 - Modificar un nodo.}
    \label{tab:04-req03}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.5\textwidth}| }
        \hline
        Nombre                 & Eliminar un nodo                              \\
        \hline
        Objetivos relacionados & \ref{tab:04-obj01}                            \\
        \hline
        Descripción            & Un usuario debe poder eliminar del sistema un
        nodo cuando sea necesario.                                             \\
        \hline
    \end{tabular}
    \caption{Requisito 04 - Eliminar un nodo.}
    \label{tab:04-req04}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.5\textwidth}| }
        \hline
        Nombre                 & Añadir una nueva tarea                         \\
        \hline
        Objetivos relacionados & \ref{tab:04-obj02}                             \\
        \hline
        Descripción            & Un usuario debe poder añadir al sistema tareas
        de tiempo real para su posterior despliegue sobre los nodos del mismo.
        Se deben aportar, por tanto, tanto los ficheros con la tarea como los
        atributos relativos a sus restricciones temporales.                     \\
        \hline
    \end{tabular}
    \caption{Requisito 05 - Añadir una nueva tarea.}
    \label{tab:04-req05}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.5\textwidth}| }
        \hline
        Nombre                 & Obtener datos de las tareas              \\
        \hline
        Objetivos relacionados & \ref{tab:04-obj02}                       \\
        \hline
        Descripción            & Un usuario debe poder ver la información
        relativa a las tareas registradas en el sistema.                  \\
        \hline
    \end{tabular}
    \caption{Requisito 06 - Obtener datos de las tareas.}
    \label{tab:04-req06}
\end{table}


\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.5\textwidth}| }
        \hline
        Nombre                 & Modificar una tarea                           \\
        \hline
        Objetivos relacionados & \ref{tab:04-obj02}                            \\
        \hline
        Descripción            & Un usuario debe poder actualizar una tarea ya
        presente en el sistema si fuera necesario.                             \\
        \hline
    \end{tabular}
    \caption{Requisito 07 - Modificar una tarea.}
    \label{tab:04-req07}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.5\textwidth}| }
        \hline
        Nombre                 & Eliminar una tarea                           \\
        \hline
        Objetivos relacionados & \ref{tab:04-obj02}                           \\
        \hline
        Descripción            & Un usuario debe poder eliminar una tarea del
        sistema.                                                              \\
        \hline
    \end{tabular}
    \caption{Requisito 08 - Eliminar una tarea.}
    \label{tab:04-req08}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.5\textwidth}| }
        \hline
        Nombre                 & Desplegar una tarea en un nodo            \\
        \hline
        Objetivos relacionados & \ref{tab:04-obj03}                        \\
        \hline
        Descripción            & Un usuario debe poder desplegar una tarea
        registrada en el sistema sobre un nodo también registrado.         \\
        \hline
    \end{tabular}
    \caption{Requisito 09 - Desplegar una tarea en un nodo.}
    \label{tab:04-req09}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.5\textwidth}| }
        \hline
        Nombre                 & Eliminar una tarea de un nodo            \\
        \hline
        Objetivos relacionados & \ref{tab:04-obj03}                       \\
        \hline
        Descripción            & Un usuario debe poder eliminar una tarea
        previamente desplegada en un nodo cuando sea necesario.           \\
        \hline
    \end{tabular}
    \caption{Requisito 10 - Eliminar una tarea de un nodo.}
    \label{tab:04-req10}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.5\textwidth}| }
        \hline
        Nombre                 & Garantizar la viabilidad de los conjuntos de tareas \\
        \hline
        Objetivos relacionados & \ref{tab:04-obj03}                                  \\
        \hline
        Descripción            & El sistema debe de garantizar en todo momento y
        en la medida de lo posible que las tareas que se ejecutan sobre un nodo
        dado son viables.                                                            \\
        \hline
    \end{tabular}
    \caption{Requisito 11 - Garantizar la viabilidad de los conjuntos de tareas.}
    \label{tab:04-req11}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.5\textwidth}| }
        \hline
        Nombre                 & Despliegue usando contenedores                 \\
        \hline
        Objetivos relacionados & \ref{tab:04-obj03}                             \\
        \hline
        Descripción            & El despliegue de las tareas sobre los nodos se
        debe realizar mediante contenedores.                                    \\
        \hline
    \end{tabular}
    \caption{Requisito 12 - Despliegue usando contenedores.}
    \label{tab:04-req12}
\end{table}

\section{Diseño del sistema}
\label{sec:04-system_design}

A la hora de abordar el diseño de la herramienta, se nos planteaban varias
posibilidades. La solución más sencilla podría consistir de una aplicación
monolítica, de escritorio o web, que aglutinase toda la funcionalidad recogida
en los requisitos detallados en la sección anterior. Una aplicación monolítica
de este tipo, aunque no siempre es una mala opción, plantea varios problemas
como son su difícil escalabilidad o la complejidad creciente conforme se añaden
funcionalidades, lo que incrementa los costes de desarrollo y mantenimiento.
Además, el sistema que se desea crear se visualiza siendo usado por varios
usuarios. Se podría pensar, por ejemplo, en ingenieros de una fábrica que se
encargan del desarrollo y mantenimiento de sus sistemas de control, cada uno
usando su propio PC para llevar a cabo estas tareas a través de nuestra
herramienta. En una situación como esa, usar una aplicación monolítica es
inviable, dado que los usuarios deben trabajar de forma concurrente sobre un
conjunto de datos común.

La manera más sencilla para permitir esta colaboración y conseguir que todos los
usuarios trabajen sobre una misma base consiste en aplicar un modelo de
cliente-servidor. El sistema se modulariza, encapsulando la funcionalidad más
importante en un servidor y relegando las aplicaciones que usan los usuarios a
simples cascarones encargados de proporcionar una interfaz entre las
funcionalidades del sistema y ellos mismos. Una arquitectura de este tipo hace
más fácil controlar el acceso concurrente a los datos, evitando problemas de
integridad e incoherencia en los mismos. Además, debido a que los clientes no
tienen tantas responsabilidades, acaban siendo más simples y livianos, lo que
facilita su instalación en las máquinas de los usuarios. Es por todas estas
razones que se ha escogido la arquitectura cliente-servidor para el diseño del
sistema. En la figura \ref{fig:04-architecture} se puede ver un diagrama del
diseño propuesto para el sistema. Aquí se puede ver como el servidor contiene
toda la lógica relativa a los nodos, las tareas y el despliegue de éstas. Aunque
se podría haber planteado la separación de esta lógica en varios microservicios,
esta opción se descartó debido a que no se espera que el sistema tenga que ser
usado por muchos usuarios a la vez ni su carga pueda elevarse de forma
considerable, por lo que los beneficios de escalabilidad que este tipo de diseño
podría aportar son innecesarios y se ven enormemente contrarrestados por la
complejidad que añaden.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{04-implementation/architecture.png}
    \caption{Diagrama de arquitectura del sistema.}
    \label{fig:04-architecture}
\end{figure}

Volviendo a la figura \ref{fig:04-architecture}, apreciamos que el servidor
también controla el acceso a la base de datos donde se almacena toda la
información sobre los nodos y las tareas. Para esta base de datos, como ya se
indicó en la sección \ref{sec:01-tools}, se ha decidido MongoDB, que es un gestor
de bases de datos documentales y no estructuradas. También se puede comprobar en
el diagrama que el servidor expone todas las funcionalidades del sistema a
través de una API (\textit{Application Programming Interface}). Esta API sigue
el patrón arquitectónico REST (\textit{REpresentation State Transfer}), el cuál
fue introducido en el año 2000 por Roy Fielding en su tesis doctoral
\cite{fielding_architectural_2000}. Simplificando los conceptos que se
desarrollan en esta tesis, podemos identificar las siguientes características
fundamentales del patrón REST:

\begin{itemize}
    \item La interfaz se diseña en torno a recursos, los cuáles son conjuntos de
          información.
    \item Cada recurso está identificado de forma única mediante una URI.
    \item Las operaciones se realizan mediante peticiones HTTP hacia la URI del
          recurso objetivo.
    \item La operación a realizar viene definida por el verbo HTTP usado en la
          petición (\texttt{GET}, \texttt{POST}, \texttt{PUT}, \texttt{PATCH} o
          \texttt{DELETE}).
    \item El servidor no almacena estado alguno, de forma que cada petición debe
          contener toda la información necesaria como para poder darle respuesta.
    \item Para la transferencia de información, se utilizan formatos bien
          definidos como JSON o XML.
\end{itemize}

La aplicación del patrón REST al diseño de interfaces hace que sea más sencillo
estructurar el acceso a la información que contiene un servicio concreto, además
de simplificar también el flujo de trabajo entre cliente y servidor. Para
aplicar REST al diseño de un servicio concreto, se debe identificar cuáles son
los recursos con los que trabaja y qué operaciones permite sobre los mismos. En
el caso de nuestro sistema, tenemos los recursos detallados en las tablas
\ref{tab:04-node_resource} y \ref{tab:04-task_resource}, mientras que las
operaciones posibles se presentan en la sección \ref{sec:server-design}.

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.8\textwidth}| }
        \hline
        Recurso   & Nodo           \\
        \hline
        Atributos &
        \begin{itemize}
            \item ID
            \item Nombre
            \item Dirección IP
            \item Usuario (utilizado para la conexión SSH)
            \item CPU (modelo del procesador)
            \item Arquitectura de CPU (p. ej., \texttt{ARMv7})
            \item Número de núcleos de la CPU
            \item Frecuencia de reloj
            \item RAM
            \item Dispositivos (p. ej., \texttt{/dev/null})
            \item Tareas (que se están ejecutando en el nodo)
        \end{itemize} \\
        \hline
    \end{tabular}
    \caption{Definición del recurso nodo.}
    \label{tab:04-node_resource}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ |>{\columncolor[gray]{0.8}}l|p{0.8\textwidth}| }
        \hline
        Recurso   & Tarea          \\
        \hline
        Atributos &
        \begin{itemize}
            \item ID
            \item Nombre
            \item Tiempo de ejecución (\textit{runtime})
            \item Límite de tiempo (\textit{deadline})
            \item Período (\textit{period})
            \item Dispositivos (a los que necesita acceder)
            \item Capacidades (p. ej., \texttt{SYS\_NICE})
            \item ID del desplegable (fichero tar con código fuente y \texttt{Dockerfile})
        \end{itemize} \\
        \hline
    \end{tabular}
    \caption{Definición del recurso tarea.}
    \label{tab:04-task_resource}
\end{table}

Como ya se ha indicado, aplicar una arquitectura cliente-servidor como esta
permite reducir al máximo la lógica que contienen los clientes, los cuáles solo
tienen que ocuparse de mostrar la interfaz, comprobar las entradas de los
usuarios y realizar la comunicación con el servidor. Además, el hecho de que el
servidor exponga una API estandarizada facilita enormemente la implementación de
múltiples tipos de aplicaciones cliente, incluso para distintas plataformas (p.
ej., móvil, web o escritorio). Estos clientes sencillamente tienen que consumir
recursos de la interfaz y pueden ofrecer a sus usuarios exactamente las mismas
funcionalidades que los demás. Esta separación de responsabilidades facilita
también el desarrollo y mantenimiento del software, ya que se trata de partes
completamente independientes y separadas que solo deben atenerse a la
especificación de la interfaz mediante la que se conectan.

Por último, también se ha decidido en esta fase de diseño implementar una imagen
Docker, que es el motor de contenedores usado por el sistema, para facilitar el
despliegue de tareas de tiempo real usando el sistema planteado. El
funcionamiento de esta imagen, así como los detalles de diseño e implementación
de las demás partes del sistema, se explican de forma extensiva en las
siguientes secciones de este capítulo. Todo el código desarrollado del que se va
a hablar se encuentra disponible en los repositorios de GitHub del
proyecto\footnote{Repositorio del servidor:
    \url{https://github.com/Varrrro/shipyard-server}}\footnote{Repositorio del
    cliente: \url{https://github.com/Varrrro/shipyard-cli}}\footnote{Repositorio de
    la librería: \url{https://github.com/Varrrro/libshipyard}}.

\section{Servidor maestro}

Tal y como se ha descrito en la sección anterior, el servidor es la parte del
sistema desarrollado que encapsula la lógica principal encargada de llevar a
cabo las funcionalidades deseadas. En esta sección se ahonda en las decisiones
de diseño tomadas para este componente, además de mostrar algunas partes
interesantes de la implementación y las técnicas de prueba e integración
continua aplicadas.

\subsection{Diseño e implementación}
\label{sec:server-design}

A la hora de estructurar el servidor, decidimos hacer una separación por
dominios, de forma que tenemos por un lado la lógica relativa a las
funcionalidades de los nodos y, por otro, la de las tareas. A su vez, también
realizamos una división de responsabilidades, diferenciando entre controladores
y servicios.

Los controladores definen las rutas y operaciones posibles en la API REST del
servidor. Este servidor recibe peticiones HTTP para las rutas que expone, las
cuáles se manejan con distintos controladores dependiendo del verbo HTTP usado.
Estos controladores se encargan entonces de validar los datos recibidos en la
petición, gestionar la autenticación y construir las respuestas con el código de
estado adecuado, llamando a funciones de los servicios para realizar las
operaciones. En los servicios se implementa la lógica encargada de llevar a cabo
estas operaciones como tal, conectando con la base de datos para la gestión de
nodos y tareas, además de llevar a cabo el despliegue de éstas. Como ya se ha
indicado, la lógica se divide también por dominio, de forma que tenemos unos
controladores para las operaciones sobre los nodos y otros separados para las
que ocurren sobre las tareas. Lo mismo ocurre con la lógica de estas operaciones
en los servicios. Para las acciones de orquestación de tareas, además, los
servicios hacen uso de un paquete independiente que implementa estas
funcionalidades concretas. El diagrama de la figura
\ref{fig:04-server_architecture} refleja esta arquitectura que se acaba de
describir.

En cuanto a las operaciones concretas que se pueden realizar mediante la API
REST del servidor, se han definido e implementado las siguientes:

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{04-implementation/server.png}
    \caption{Diagrama de arquitectura del servidor.}
    \label{fig:04-server_architecture}
\end{figure}

\begin{itemize}
    \item \texttt{GET /nodes}

          Con esta operación, se obtiene una lista completa de los nodos
          presentes en el sistema. En caso de ocurrir algún error (p. ej., en la
          conexión con la base de datos), el servidor devuelve un una respuesta
          500 (\textit{Internal Server Error}). Si no hay ningún problema, la
          respuesta tiene el código de estado 200 (\textit{Ok}) y contiene en su
          cuerpo la lista de nodos. Cabe destacar que en esta lista de nodos no
          se muestra toda la información presente en la base de datos sobre los
          mismos, sino que se ocultan algunos datos como son el usuario SSH, los
          dispositivos que tiene o los detalles de la CPU.

    \item \texttt{POST /nodes}

          Esta operación permite crear un nuevo nodo, debiendo incluir en el
          cuerpo de la petición los datos del nuevo nodo. No es necesario
          proporcionar valores para todos los atributos definidos en la tabla
          \ref{tab:04-node_resource}, sino que solo son requeridos el nombre,
          dirección IP y número de núcleos de la CPU del nuevo nodo. El nombre
          dado debe ser único, devolviendo el servidor una respuesta 409
          (\textit{Conflict}) si ya está en uso por otro nodo del sistema.

          La petición HTTP que recibe el servidor debe contener en la cabecera
          \textit{Authorization} unas credenciales de autenticación de tipo
          básico, es decir, usuario y contraseña codificados con Base64. Este
          usuario y contraseña son los usados por el servidor para establecer la
          conexión SSH inicial con el nodo. Esta conexión inicial sirve para
          añadir los datos del nodo al fichero \texttt{known\_hosts} del
          servidor con el que se regulan las conexiones SSH conocidas y copiar
          la clave pública de éste al fichero \texttt{authorized\_keys} del
          nodo. Esto permite que las siguientes conexiones SSH que deba realizar
          el servidor usen esta clave para la autenticación, de forma que no sea
          necesario usar la contraseña y aumentando la seguridad.

          Si los datos enviados en el cuerpo de la petición no son correctos, el
          controlador devuelve una respuesta 400 (\textit{Bad Request}). En caso
          de no poder crear el nodo por cualquier otro motivo, se devuelve un
          código 500. La ejecución correcta de esta operación produce una
          respuesta con código de estado 200 y con el nuevo recurso recién
          creado en el cuerpo.

    \item \texttt{GET /nodes?name=<node\_name>}

          Como se puede suponer por el verbo HTTP y la ruta usadas, esta
          operación permite obtener la información detallada de un nodo
          concreto, el cuál se identifica por el nombre proporcionado. Esta
          operación y \texttt{GET /nodes} comparten controlador, comprobándose
          al inicio de éste si en la URI de la petición se encuentra el
          parámetro de consulta \textit{name} para decidir si se llama a la
          función del servicio de nodos que devuelve la lista o a la que
          devuelve los detalles de uno concreto. A diferencia de lo que ocurría
          con esta otra operación, en este caso sí que se devuelven todos los
          atributos del nodo indicado junto con la respuesta 200. Si no se
          encuentra en la base de datos ningún nodo con el nombre indicado, la
          respuesta devuelta tiene un código de estado 404 (\textit{Not Found}).

    \item \texttt{GET /nodes/<node\_id>}

          Esta operación es idéntica a la anterior, siendo la única diferencia
          que la identificación del nodo cuyos detalles se desean obtener se
          realiza por su ID. Esta operación también devuelve una respuesta 200
          con los datos del nodo en el cuerpo en caso de encontrarse en la base
          de datos y un código 404 en caso contrario. También puede devolver una
          raspuesta 409 si el ID dado no es válido, es decir, si no cumple con
          el formato de ID de MongoDB.

    \item \texttt{PUT /nodes/<node\_id>}

          Con esta operación se pueden modificar los atributos de un nodo ya
          existente en el sistema. En el cuerpo de la petición se incluyen solo
          los atributos cuyo valor se desea cambiar junto con estos nuevos
          valores. Si el formato del cuerpo de la petición no es correcto, el
          controlador de la ruta devuelve una respuesta 409. También se devuelve
          este código si el ID indicado no es válido. Si no se encuentra en la
          base de datos del sistema ningún nodo con el ID dado, se devuelve una
          respuesta 404. En caso de realizarse correctamente la operación, la
          respuesta tiene el código de estado 200 y contiene en su cuerpo todos
          los datos del recurso actualizado. Cabe destacar que, si alguno de los
          atributos modificados es el nombre, la IP, el usuario SSH o la lista
          de dispositivos del nodo, el servidor procede a eliminar del mismo
          todas las tareas que tuviera ejecutándose con el fin de evitar
          inconsistencias y estados no recuperables en el sistema. En el
          diagrama de la figura \ref{fig:04-update_node} se refleja este
          comportamiento, además del resto de acciones que lleva a cabo el
          servidor para realizar esta operación.

          \begin{figure}
              \centering
              \includegraphics[width=\textwidth]{04-implementation/update-node.png}
              \caption{Diagrama de secuencias de la operación de actualización de un nodo.}
              \label{fig:04-update_node}
          \end{figure}

    \item \texttt{DELETE /nodes/<node\_id>}

          Esta operación permite eliminar un nodo del sistema. Las peticiones no
          necesitan contener ningún dato en el cuerpo, siendo solo necesario
          indicar en la propia ruta de la petición el ID del nodo a eliminar. Si
          este ID no es válido, el controlador devuelve una respuesta 409,
          mientras que si no se encuentra ningún nodo con este ID la respuesta
          es 404. Si la eliminación del recurso se realiza correctamente, la
          respuesta tiene el código de estado 200 y contiene en su cuerpo los
          datos del recurso recién eliminado. Al igual que ocurría con la
          modificación, el sistema se asegura de parar todas las tareas que se
          estuvieran ejecutando sobre el nodo que se elimina y borrarlas del
          mismo, de forma que no queden «restos» en el dispostivo.

    \item \texttt{GET /tasks}

          De forma análoga a la operación \texttt{GET /nodes}, ésta devuelve la
          lista de tareas que están almacenadas en la base de datos del sistema.
          El cuerpo de la respuesta contiene esta lista con el código de estado
          200 si la operación se ha podido realizar correctamente, devolviendo
          el código 500 en caso contrario.

    \item \texttt{POST /tasks}

          Permite añadir una nueva tarea al sistema. A diferencia de las demás
          operaciones, la cabecera \textit{content-type} de las peticiones para
          esta operación, que es la que regula el tipo de contenido, no tiene el
          valor \texttt{application/json}, sino que se usa
          \texttt{multipart/form-data}. Este tipo de contenido se compone de
          múltiples campos, los cuáles pueden ser textuales o archivos, lo cuál
          es necesario para esta operación dado que es necesario aportar tanto
          los metadatos de la tarea en formato JSON como un fichero
          \texttt{tar.gz} con el código fuente y la definición de la imagen
          Docker. El controlador de esta operación se encarga de extraer todos
          estos datos de los dos campos del cuerpo de la petición y, entonces,
          llamar a la función del servicio encargada de realizar la inserción de
          la tarea en la base de datos, devolviendo el controlador una respuesta
          200 si la operación se realiza correctamente. Si el formato de la
          petición recibida es incorrecto (p. ej., falta alguno de los campos),
          se devuelve una respuesta 400. Además, el nombre de la nueva tarea
          debe ser único, obteniendo una respuesta 409 en caso de no serlo. Si
          no se puede crear la tarea por cualquier otra razón, la respuesta
          obtenida es 500.

    \item \texttt{GET /tasks?name=<task\_name>}

          Con esta operación se pueden obtener los detalles de una tarea
          concreta, la cuál se identifica a partir del nombre indicado en el
          parámetro de consulta \textit{name} de la petición. Al igual que
          ocurría con la operación \texttt{GET /nodes?name=<node\_name>}, el
          controlador que da respuesta a esta operación es compartido con el de
          \texttt{GET /tasks}. Al inicio de la ejecución, el controlador
          comprueba la presencia del parámetro \textit{name} en la petición,
          devolviendo los detalles de la tarea o la lista completa de tareas
          según si está presente o no. En caso de operación correcta, la
          respuesta 200 incluye en el cuerpo los datos del recurso deseado a
          excepción del fichero \texttt{tar.gz} con su código fuente. Si este
          recurso no se encuentra, el código devuelto es 404, devolviendo 500 en
          cualquier otro caso.

    \item \texttt{GET /tasks/<task\_id>}

          Al igual que la operación anterior, ésta también devuelve los detalles
          de una tarea, aunque en este caso se usa el ID de la misma para su
          identificación. Si este ID no es válido, el controlador devuelve una
          respuesta 400; mientras que si no se encuentra en la base de datos
          ninguna tarea con este ID, la respuesta es 404. Esta operación tampoco
          devuelve el fichero de despliegue asociado a la tarea. Si no se puede
          realizar la operación, el controlador devuelve una respuesta 500.

    \item \texttt{PUT /tasks/<task\_id>}

          Modificación de una tarea. Como ocurría en la operación de creación de
          tareas, las peticiones también tienen el tipo de contenido
          \texttt{multipart/form-data}. Como diferencia, no es necesario que la
          petición contenga tanto el dichero \texttt{tar.gz} con el código
          fuente como el JSON con los atributos, sino que solo debe contener lo
          que se vaya a actualizar. Antes de modificar la tarea, el servidor se
          asegura de eliminarla de cualquier nodo en el que se estuviera
          ejecutando para evitar inconsistencias en el sistema. Si el ID dado o
          el formato del cuerpo de la petición no son válidos, el controlador
          devuelve una respuesta 400. Si no se encuentra en la base de datos
          ninguna tarea con el ID indicado, se devuelve un 404. En caso de
          realizarse la operación correctamente, la respuesta devuelta tiene el
          código 200 y contiene en su cuerpo el recurso actualizado. Si no se
          puede realizar la operación por cualquier otra razón, devuelve una
          respuesta 500.

    \item \texttt{DELETE /tasks/<task\_id>}

          Elimina una tarea de la base de datos del sistema, identificada por el
          ID indicado en la propia petición. Si no se encuentra ninguna tarea
          con dicho ID, devuelve 404. Si este ID no es válido, devuelve 400. Si
          no se puede realizar la operación por cualquier otra razón, devuelve
          500. Cuando la eliminación se completa de forma satisfactoria, el
          controlador devuelve una respuesta con código 200 y con el recurso
          recién eliminado en el cuerpo.

    \item \texttt{POST /nodes/<node\_id>/tasks?task\_id=<task\_id>}

          Esta operación permite el despliegue de una tarea dada en un nodo. Es,
          quizás, la operación más compleja que realiza el servidor. Se recibe
          una petición en cuya ruta se se encuentra el ID del nodo, aportándose
          el de la tarea como un parámetro de consulta (\textit{query}). Si este
          parámetro de consulta no está presente en la petición, el controlador
          devuelve inmediatamente una respuesta 400. Si todos los valores están
          presentes, se llama entonces a la función del servicio encargada de
          llevar a cabo el despliegue.

          Esta función convierte los IDs a los propios de MongoDB, lanzando un
          error \texttt{InvalidId} en caso de que su formato no sea correcto. El
          controlador devolvería entonces una respuesta 400 al cliente. El
          servicio puede devolver un error \texttt{NotFound} si no encuentra en
          la base de datos los recursos deseados, error que captura el
          controlador para generar una respuesta 404. Si los IDs se corresponden
          con recursos existentes en el sistema, el servicio procede a comprobar
          que el nodo posea todos los dispositivos que necesita la tarea. Si no
          es así, el servicio lanza un error \texttt{MissingDevices} que es
          capturado por el controlador, que genera una respuesta 500. La última
          comprobación que se realiza es la de la viabilidad de la ejecución del
          conjunto de tareas que se obtendría al añadir la nueva tarea a las que
          ya se están ejecutando en el nodo. Tanto para realizar esta
          comprobación como para llevar a cabo el despliegue como tal, el
          servicio se apoya en el paquete interno \textit{crane} que ya fue
          introducido al inicio de esta sección. La función
          \texttt{check\_feasibility} de este paquete es comprueba esta
          viabilidad. Esta función recibe el conjunto de tareas y el número de
          núcleos de la CPU del nodo objetivo. Con estos datos, se calcula la
          utilización de CPU para determinar si la ejecución es viable o no
          \cite{zhang_schedulability_2009}. Cabe destacar que, si el nodo tiene
          más de un núcleo (multiprocesador), esta comprobación de la
          utilización no es condición suficiente para asegurar la viabilidad del
          conjunto de tareas, aunque en esta primera versión de la herramienta
          no se gestiona esta situación.

          \begin{figure}
              \centering
              \includegraphics[width=\textwidth]{04-implementation/deploy-task.png}
              \caption{Diagrama de secuencias de la operación de despliegue de una tarea.}
              \label{fig:04-deploy_task}
          \end{figure}

          Si la función \texttt{check\_feasibility} determina que el conjunto de
          tareas es viable, entonces se ejecuta otra función del paquete
          \textit{crane} para realizar el despliegue: \texttt{deploy\_task}.
          Esta función recibe como entrada el fichero \texttt{tar.gz} de la
          tarea, así como sus metadatos y los del nodo. Con estos datos, se
          establece una conexión con el demonio Docker del nodo mediante SSH,
          obteniendo un cliente con el que se construye la imagen a partir del
          fichero \texttt{tar.gz} y se lanza el contenedor. Los parámetros de
          planificación de la tarea (tiempo de ejecución, límite de tiempo y
          período) se le pasan al contenedor como variables de entorno, las
          cuáles luego usará la librería para fijar la planificación.

          Cualquier tipo de excepción que se lance durante este despliegue será
          capturada por el controlador para generar una respuesta 500. Si la
          operación se realiza correctamente, el servidor responde con una
          respuesta 200 que contiene en su cuerpo los detalles del nodo
          actualizado, incluyendo ya en su lista de tareas la que se acaba de
          añadir. Todo este proceso que se acaba de describir para realizar la
          operación de despliegue se puede observar de manera esquematizada en
          el diagrama de secuencias de la figura \ref{fig:04-deploy_task}.

    \item \texttt{DELETE /nodes/<node\_id>/tasks/<task\_id>}

          Esta operación permite parar la ejecución de una tarea desplegada en
          un nodo concreto, elmininándola del mismo. Las peticiones recibidas
          para esta operación contienen en la ruta los valores de los IDs del
          nodo y la tarea en cuestión. El controlador entonces llama a la
          función del servicio encargada de realizar la operación.

          Lo primero que hace el servicio es comprobar si están presentes en la
          base de datos algunos recursos con los IDs proporcionados, lanzando un
          error \texttt{NotFound} en caso de no encontrarlos. Este error es
          entonces capturado por el controlador, que genera una respuesta 404
          para el cliente. Si al buscar los recursos en la base de datos resulta
          que los IDs no son válidos, se lanza un error \texttt{InvalidId}, para
          el que el controlador crea una respuesta 400.

          Si los recursos existen, el servicio entonces ejecuta la función
          \texttt{remove\_task} del paquete \textit{crane} para que se lleve a
          cabo la eliminación. Al igual que ocurría en la de despliegue de
          tareas, se crea un cliente para el servidor Docker del nodo mediante
          SSH usando las claves almacenadas. A través de este cliente, se
          elimina el contenedor en el que se ejecuta la tarea y, también, la
          imagen asociada. Una vez termina esta operación, se actualiza la base
          de datos para quitar la tarea de la lista que tiene asociada el nodo.
          Si durante la eliminación ocurre algún error, el controlador la
          captura y devuelve una respuesta 500 al cliente. Si la operación se
          realiza correctamente, la respuesta tiene un código de estado 200 y
          contiene en su cuerpo los datos actualizados del nodo, ya sin la tarea
          en su lista de tareas.
\end{itemize}

Gracias al uso de la librería \textit{hug}, la implementación de los
controladores es extremadamente sencilla. Cada controlador es una simple función
Python que, mediante decoradores, se enlaza con una operación concreta del
servidor a la que da respuesta. En el listado \ref{lst:04-post_node_tasks}, se
presenta el controlador de la operación de despligue de tareas. Como se puede
observar, es con el decorador \lstinline{@hug.post} con el que indicamos que se
debe ejecutar esta función cuando se reciba una petición POST para la ruta
\texttt{/nodes/<node\_id/tasks}\footnote{La ruta \texttt{/nodes} es la base para
    todos los controladores de los nodos, por eso no se muestra en el decorador.}.
Se definen aquí también las variables de ruta presentes, que en este caso es el
ID del nodo.

\begin{lstlisting}[
    language=Python,
    caption={Controlador de la operación de despliegue de tareas.},
    label={lst:04-post_node_tasks},
    showstringspaces=false
]
@hug.post('/{node_id}/tasks')
def post_node_tasks(node_id: str, response, task_id: str = None):
    if task_id is None:
        response.status = hug.HTTP_BAD_REQUEST
        return {
            'error': 'No task ID was specified in the request'
        }

    try:
        result = NodeService.add_task(node_id, task_id)
        if result is None:
            response.status = hug.HTTP_INTERNAL_SERVER_ERROR
            return {'error': 'Unable to add task to node.'}
        return Node.Schema().dump(result)
    except InvalidId as e:
        response.status = hug.HTTP_BAD_REQUEST
        return {'error': str(e)}
    except NotFound as e:
        response.status = hug.HTTP_NOT_FOUND
        return {'error': str(e)}
    except (NotFeasible, MissingDevices) as e:
        response.status = hug.HTTP_INTERNAL_SERVER_ERROR
        return {'error': str(e)}
    except Exception:
        response.status = hug.HTTP_INTERNAL_SERVER_ERROR
        return {'error': 'Unable to add task to node.'}   
\end{lstlisting}

Las funciones definidas como controladores con \textit{hug} reciben
estas variables de ruta como argumentos, teniendo también los posibles
parámetros de consulta (\texttt{task\_id}) y los objetos propios de la
comunicación HTTP (p. ej., \texttt{request}, \texttt{response}, \texttt{body}).
Observando el listado, se aprecia claramente cómo los controladores se encargan
solo de comprobar que el formato de la petición sea el correcto, llamar a la
función del servicio apropiada para realizar la operación y construir la
respuesta que corresponda en base al resultado de la misma, tal y como se
describía anteriormente.

\begin{lstlisting}[
    language=Python,
    caption={Función del servicio de tareas para la creación de tareas.},
    label={lst:04-task_service_create},
    showstringspaces=false
]
@staticmethod
def create(
    new_task: Task,
    file_name: str,
    file_body: BytesIO
    ) -> str:

    result = db.tasks.find_one({'name': new_task.name})
    if result is not None:
        raise AlreadyPresent(
            'A task already exists with the given name.'
        )

    file_id = fs.put(file_body, filename=file_name)
    new_task.file_id = file_id

    new_id = db.tasks.insert_one(Task.Schema(
        exclude=['_id']).dump(new_task)).inserted_id
    return str(new_id)
\end{lstlisting}

Por otro lado, en el listado \ref{lst:04-task_service_create} se muestra la
función del servicio de tareas que implementa la lógica encargada de la creación
de éstas. El objeto \texttt{db}, como se puede suponer, es la conexión con la
base de datos que mantiene el servidor. Esta función concreta tiene una
particularidad con respecto a las demás: se debe almacenar también el fichero
\texttt{tar.gz} con el código fuente y la definición de imagen de la tarea. Para
ello, usamos GridFS, que es la especificación de MongoDB para el almacenamiento
de ficheros de gran tamaño. De esta forma, se pueden almacenar los ficheros en
colecciones normales de MongoDB sin necesitar de otro tipo de almacenamiento. Al
igual que el objeto \texttt{db} era la conexión con la base de datos MongoDB,
\texttt{fs} es la conexión con la parte de GridFS. Entonces, para añadir una
nueva tarea al sistema, primero se añade el fichero con \texttt{fs.put()} y, con
el ID recién obtenido para el mismo, se crea el nuevo documento en la colección
de tareas.

Otro aspecto a destacar es el uso de \texttt{BytesIO} para el acceso al fichero.
Cuando el controlador de la operación \texttt{POST /tasks} recibe una nueva
petición, extrae el contenido del fichero del cuerpo de la misma y lo escribe a
un objeto de este tipo. A todos los efectos, un objeto \texttt{BytesIO} actúa
como un archivo normal, con la diferencia de que sus datos se almacenan
directamente en memoria en vez de en el sistema de archivos. Al usar este
mecanismo, conseguimos hacer que el proceso de inserción de la tarea sea más
rápido, dado que no tenemos que acceder a disco duro ni tenemos que trabajar con
ficheros.

Como se ha indicado en la sección \ref{sec:04-system_design}, la API REST usa el
formato estándar de codificación de datos JSON para la transmisión de
información entre el servidor y los clientes. La mayoría de las peticiones y
respuetas contienen en sus cuerpos una representación de los recursos del
sistema en este formato. Para que la serialización de los objetos Python a JSON
y viceversa se realice de forma más sencilla, se ha usado la librería
\textit{marshmallow}. Esta librería nos permite definir esquemas de datos para
los recursos del sistema que, además de ayudar con la serialización y
deserialización, proporciona un mecanismo de validación automática sobre los
datos recibidos de los clientes.

\subsection{Prueba}

Para asegurar el correcto funcionamiento del código implementado, se han escrito
pruebas unitarias. Este tipo de pruebas se centran en validar el comportamiento
de partes individuales del sistema, aíslandolas del resto. En nuestro caso, las
pruebas se realizan para las funciones de los controladores y los servicios. La
librería \textit{unittest}, que forma parte de la librería estándar de Python,
proporciona los mecanismos que se han utilizado para implementar estas pruebas.
Con esta librería, se definen clases \texttt{TestCase}, que son las unidades
individuales de prueba para un componente concreto. Las funciones de estas
clases son las que comprueban que las salidas obtenidas para determinadas
entradas son las correctas en las funciones individuales del componente dado.
Para nuestro servidor, se han definido cuatro clases \texttt{TestCase} para los
cuatro componentes principales: controladores de los nodos, controladores de las
tareas, servicio de nodos y servicio de tareas.

\begin{lstlisting}[
    language=Python,
    caption={Prueba unitaria del controlador para el despliegue de tareas.},
    label={lst:04-post_node_tasks_test}
]
def test_post_node_tasks(self):
    response = hug.test.call(
        'POST', controllers, f'{test_nodes[0]._id}/tasks',
        params={
            'task_id': 'Test'
    })
    self.assertEqual(response.status, hug.HTTP_OK)
    self.assertIsNotNone(response.data)

    response = hug.test.call(
        'POST', controllers, f'{ObjectId()}/tasks', params={
            'task_id': 'Test'
    })
    self.assertEqual(response.status, hug.HTTP_NOT_FOUND)
    self.assertIsNotNone(response.data)
    self.assertIsInstance(response.data['error'], str)

    response = hug.test.call(
        'POST', controllers, f'{test_nodes[0]._id}/tasks')
    self.assertEqual(response.status, hug.HTTP_BAD_REQUEST)
    self.assertIsNotNone(response.data)
    self.assertIsInstance(response.data['error'], str)

    response = hug.test.call(
        'POST', controllers, f'error/tasks', params={
            'task_id': 'Test'
    })
    self.assertEqual(response.status, hug.HTTP_BAD_REQUEST)
    self.assertIsNotNone(response.data)
    self.assertIsInstance(response.data['error'], str)

    response = hug.test.call(
        'POST', controllers, f'{test_nodes[0]._id}/tasks',
        params={
            'task_id': 'MissingDevices'
    })
    self.assertEqual(
        response.status, hug.HTTP_INTERNAL_SERVER_ERROR)
    self.assertIsNotNone(response.data)
    self.assertIsInstance(response.data['error'], str)

    response = hug.test.call(
        'POST', controllers, f'{test_nodes[0]._id}/tasks',
        params={
            'task_id': 'NotFeasible'
    })
    self.assertEqual(
        response.status, hug.HTTP_INTERNAL_SERVER_ERROR)
    self.assertIsNotNone(response.data)
    self.assertIsInstance(response.data['error'], str)
\end{lstlisting}

Como se acaba de explicar, una prueba unitaria valida una única parte del código
de forma aislada. En nuestro servidor, tanto los servicios como los
controladores deben interactuar con otros componentes para cumplir su función.
Para poder probar solo una función concreta, necesitamos hacer maquetas o
\textit{mocks} de los demás componentes con los que interactúa. Estas maquetas
son implementaciones de prueba, en muchos casos vacías o con un comportamiento
concreto para cada caso de prueba. Por ejemplo, para poder probar los
controladores, necesitamos una maqueta del servicio con el que trabajan. La
maqueta ofrece una implementación «de juguete» con un comportamiento igual al
que tendría el servicio real. La librería \textit{unittest} proporciona el
decorador \texttt{@mock.patch} para las clases \texttt{TestCase}, con el cuál se
puede proporcionar una maqueta para que sea usada por el código que se prueba en
lugar del elemento real.

En el listado \ref{lst:04-post_node_tasks_test} se puede ver la función de
prueba para el controlador de despliegue de tareas del \texttt{TestCase} de los
controladores de los nodos. En \ref{lst:04-post_node_tasks} se puede ver como
este controlador llama a la función \texttt{add\_task} del servicio de nodos. La
maqueta del servicio que se usa para esta prueba tiene una implementación simple
de esta función que, dependiendo de la entrada dada, permite obtener todas los
resultados que se podrían obtener con la implementación real. De esta forma,
podemos comprobar que el controlador se comporte correctamente para cada una de
estas salidas.

Volviendo a la prueba de \ref{lst:04-post_node_tasks_test}, consiste
sencillamente de una serie de peticiones con distintos valores que se envían al
controlador gracias a la función \texttt{hug.test.call()} que la propia librería
\textit{hug} proporciona para escribir pruebas. Para cada petición, se comprueba
que la respuesta tiene el código de estado HTTP correcto, además de los valores
deseados en el cuerpo.

\begin{lstlisting}[
    language=Python,
    caption={Prueba unitaria de la función de creación de tareas.},
    label={lst:04-task_service_create_test}
]
def test_create(self):
    new_task = Task.Schema().load({
        'name': 'Test',
        'runtime': 1000,
        'deadline': 1000,
        'period': 1000
    })

    try:
        result = TaskService.create(
            new_task, 'test_file.tar.gz', BytesIO())
        self.assertEqual(
            mockdb.tasks.count_documents({}), len(test_tasks)+1)
        self.assertIsInstance(result, str)
    except:
        self.fail()

    with self.assertRaises(AlreadyPresent):
        TaskService.create(
            new_task, 'test_file.tar.gz', BytesIO())
    self.assertEqual(
        mockdb.tasks.count_documents({}), len(test_tasks)+1)
\end{lstlisting}

Los servicios, por otro lado, interactúan tanto con la base de datos MongoDB
como con las funciones de la librería interna \textit{crane}. Estas funciones se
pueden maquetar al igual que hacíamos con las de los propios servicios para
probar los controladores dado que conocemos su implementación y su alcance es
pequeño. La conexión con la base de datos plantea una situación diferente.
Podríamos levantar una instancia de MongoDB cada vez que quisiéramos ejecutar
las pruebas, aunque esto rompe ligeramente con el enfoque aislado de las pruebas
unitarias. Además, esto tiene un impacto considerable en la complejidad de la
ejecución de estas pruebas, que deberían ser simples y directas. Por ello,
hacemos uso de la librería \textit{mongomock}, que nos proporciona una maqueta
de los clientes de MongoDB usados por los servicios. Estas maquetas funcionan
exactamente igual que los clientes normales, almacenando los datos de la
hipotética base de datos en memoria y permitiendo comprobar que las operaciones
se realizan correctamente. Antes de ejecutar las pruebas de los servicios, se
añaden datos de prueba a estas maquetas, de forma que se simule una base de
datos real en producción.

Por lo demás, las pruebas como tal son muy similares a las de los controladores,
tal y como se puede ver en el listado \ref{lst:04-task_service_create_test}. En
él, se muestra la validación del funcionamiento de la lógica de creación de
tareas que fue expuesta en \ref{lst:04-task_service_create}, que consiste en
añadir una tarea simple y comprobar que en la base de datos maquetada
(\texttt{mockdb}) se ha añadido el nuevo documento. Se repite la inserción para
asegurar que no se pueden crear elementos duplicados.

\subsection{Integración continua}

Para facilitar el despliegue de este servidor en múltiples plataformas, se ha
definido una imagen Docker sencilla. Esta imagen se puede encontrar en el
repositorio de DockerHub\footnote{Enlace a la imagen:
    \url{https://hub.docker.com/repository/docker/varrrro/shipyard-server}}. La
construcción de la imagen se realiza de forma automática cuando se publica una
nueva versión del servidor en el repositorio de GitHub del mismo gracias a
GitHub Actions. Esta funcionalidad de GitHub permite definir secuencias de pasos
que se ejecutan cuando ocurren ciertos eventos en el repositorio.

Además de construir la imagen y publicarla en DockerHub para cada nueva
\textit{release}, se ha definido otra acción que ejecuta las pruebas unitarias
del proyecto de forma automática para cada nuevo cambio que se realiza sobre la
base de código del repositorio. Al ejecutar las pruebas, se obtiene además un
informe de la cobertura del código que se envía a Codecov. La cobertura mide el
porcentaje de líneas de código del proyecto que son ejecutadas por alguna
prueba. Codecov permite visualizar estos informes de manera sencilla con paneles
de control como el mostrado en la figura \ref{fig:01-codecov}. De esta forma,
podemos monitorizar la calidad del código y comprobar si los nuevos cambios
introducidos siguen pasando las pruebas de forma correcta.

\section{Cliente de terminal}

\subsection{Diseño e implementación}

\subsection{Prueba}

\subsection{Integración continua}

\section{Imagen base}

\subsection{Diseño e implementación}

\subsection{Integración continua}

\section{Análisis del rendimiento}