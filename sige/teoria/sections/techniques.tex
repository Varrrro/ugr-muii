\section{Principales técnicas de \textit{word embedding}}
\label{sec:techniques}

Como ya se ha indicado, los \textit{word embeddings} son modelos entrenados para
la obtención de representaciones vectoriales a partir de palabras. Estos modelos
pueden ser entrenados de forma individual para realizar su labor o como parte de
una tarea concreta de NLP, obteniendo unas representaciones más adaptadas al
problema específico~\cite{techniques}. De todas formas, un modelo ya entrenado
se puede reutilizar para tareas de NLP diferentes, pudiendo actualizarse para
adaptarse a las necesidades concretas de cada caso.

En las siguientes subsecciones se presentan y explican brevemente algunas de las
técnicas más conocidas de \textit{word embedding}.

\subsection{Word2vec}

\subsubsection{\textit{Continuous bag-of-words}}

\subsubsection{\textit{Continuous skip-gram}}

\subsection{GloVe}

\subsection{\textit{Embedding layer}}
