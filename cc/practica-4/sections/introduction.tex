\section{Introducción}

En esta práctica, se va resolver un problema de clasificación con una gran
cantidad de datos sobre un clúster de nodos para aprender cómo se trabaja con
\textit{Big Data} en el \textit{cloud}. El procesamiento se realiza con Spark
sobre el sistema de archivos distribuido de Hadoop. He implementado el flujo de
trabajo en Python haciendo uso de la librería \texttt{pyspark}.

En las siguientes secciones de este documento, se explica cómo se han llevado a
cabo las distintas tareas necesarias, desde la carga de datos hasta el
entrenamiento de los modelos para los distintos clasificadores. Todo ello estará
acompañado de pequeños fragmentos de código para las partes más relevantes. El
código completo se puede encontrar en el repositorio de GitHub creado para esta
práctica~\footnote{Repositorio de GitHub: \url{https://github.com/Varrrro/spark-ml}}.
